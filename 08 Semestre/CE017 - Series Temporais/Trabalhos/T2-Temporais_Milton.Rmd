---
title: "2º Trabalho - Série Temporais"
subtitle: "Parte III: 10, 18, 20, 31 e 36. Parte IV: 6"
author: "Milton Abraao Ferreira | GRR20137581"
date: '`r Sys.Date() `'
output:
  pdf_document: default
header-includes: \usepackage[brazil]{babel}
                 \usepackage{amsmath}
                 \usepackage{float}
                 \usepackage{bm}
---

# PARTE III - Modelos ARIMA

## Exercício 10

- a) Com os dados *cmort* foi ajustado um AR(2) usando regressão linear. Os dados 
  foram coletados semanalmente ao longo dos anos, por isso a frequência utilizada para 
  transformar os dados em uma série foi *52*.

```{r, results='hide'}
library(astsa)

cmort.ts = ts(cmort, freq = 52, start = c(1970,1))
par(mar=c(4,4,0,0.5));plot.ts(cmort.ts)

x.lag1 <- lag(cmort.ts, 1)
x.lag2 <- lag(cmort.ts, 2)

dbajuste = na.omit(cbind(cmort.ts,x.lag2, x.lag1))

fit <- lm(cmort.ts ~ ., data = dbajuste)
```

Pelo gráfico acima é possível observar a correlação dos erros, também na função de autocorrelação parcial (PACF) a correlação autoregressiva. 


```{r results='hide'}
par(mar=c(4,4,0,0));acf2(resid(fit))
```

Como os resíduos estão dentro do intervalo de confiança, podemos considerar que o modelo está bem ajustado.


- b) Uma maneira alternativa de ajustar o modelo de regressão como feito no exercício 
  acima é utilizando a função *sarima()*. Como neste caso estamos lidando com predição, 
  o código utilizado é o *sarima.pred()*, assim como foi feito abaixo. 
  Um ponto de atenção é que modelos lineares são mais sensíveis a grande números 
  quando comparado ao ajuste com *sarima()*.

```{r, results='hide'}
par(mar=c(4,4,0,0));sarima.for(cmort.ts, 2,0,0, n.ahead = 4)
```

  O gráfico apresentou somente o final da série, uma vez que esse período possui os dados mais relevantes para o ajuste.


## Exercício 18

Ajustando modelos para da questão (mesma base utilizada no exercício 10):

```{r}

# Ajustando o AR(2)
AR2 <- arima(cmort, order=c(2,0,0))

# Usando regressão linear
mod1 <- ar.ols(cmort, order=2, demean=FALSE, intercept=TRUE)

# Usando o Yule-Walker
mod2 <- ar.yw(cmort, order=2) 

```

- a) As estimativas ficaram muito próximas. A diferênça é que a primeira possui estimativa para o intercepto, enquanto a segunda não.

```{r}

# Usando regressão linear:
mod1$ar

# Usando Yule-Walker:
mod2$ar

```

- b) As estimativas dos erros padrões foram baixas nos dois modelos. 
  Além de baixas, elas também foram bem próximas, reafirmando assim a "Proposição III.10" 
  que diz respeito aos estimadores do modelo ARIMA para amostras grandes. 
  Em outras palavras, 
  muito provávelmente a distribuição dos estimadores para o modelo ARIMA está próxima 
  da distribuição normal assintótica.

```{r}

# Usando regressão linear:
mod1$asy.se.coef

# Usando Yule-Walker:
mod2$ar
sqrt(diag(mod2$asy.var.coef))

```


## Exercício 20

Os resultados diferem porque o modelo depende apenas dos dados amostrais realizados, 
as componentes autoregressivas e de medias móveis dependem apenas das ultimas 
realizações em cada modelo simulado, o que por si só defire naturalemente em cada 
série.

```{r}

set.seed(123)
arma_sim1 <- arima.sim(model=list(ar=c(.9),ma=c(.9)),n=500)
                      
par(mar=c(4,4,0,0));ts.plot(arma_sim1)


par(mar=c(4,4,3,0));arma_acf1 <- acf(arma_sim1,type="correlation",plot=T)
arma_acf1 

par(mar=c(4,4,3,0));arma_pacf1 <-acf(arma_sim1,type="partial",plot=T)
arma_pacf1 

ajust1 <- arima(arma_sim1, order = c(1,0,1), seasonal = c(0,0,0))

arma_sim2 <- arima.sim(model=list(ar=c(.9),ma=c(.9)),n=500)


par(mar=c(4,4,0,0));ts.plot(arma_sim2)

par(mar=c(4,4,3,0));arma_acf2 <-acf(arma_sim2,type="correlation",plot=T)
arma_acf2 

par(mar=c(4,4,3,0));arma_pacf2 <-acf(arma_sim2,type="partial",plot=T)
arma_pacf2 

ajust2 <- arima(arma_sim2, order = c(1,0,1), seasonal = c(0,0,0))

arma_sim3 <- arima.sim(model=list(ar=c(.9),ma=c(.9)),n=500)


par(mar=c(4,4,0,0));ts.plot(arma_sim3)

par(mar=c(4,4,3,0));arma_acf3 <- acf(arma_sim3,type="correlation",plot=T)
arma_acf3 

par(mar=c(4,4,3,0));arma_pacf3<-acf(arma_sim3,type="partial",plot=T)
arma_pacf3 

ajust3 <- arima(arma_sim3, order = c(1,0,1), seasonal = c(0,0,0))

```





## Exercício 31

I) Gráfico de resíduos padronizados: semelhante ao exemplo III.40, não há padrões 
claros no gráfico. Porém, devido a presença de valores discrepantes, algumas 
observações apresentam valores acima de três desvios padrões.

II) ACF dos resíduos: não há nenhuma presença de auto-correlação evidente.

III) Q-Qplot dos resíduos padronizados: há presença de valores discrepantes nas caudas. 
As observações entre -1 e 2 quantis teóricos estão bem ajustadas.

IV) p-valores da estatística Q de Ljung-Box: nenhum p-valor abaixo do nível de 
significância, evidenciando a independência entre os resíduos.

```{r, results='hide'}

gnpgr <- diff(log(gnp))

(Ex31 <- sarima(gnpgr,1,0,0))

```





## Exercício 36

- a) O comportamento da série temporal referente a variável preço mediano anual de 
  varejo por GB de discos rígidos está apresentado na figura a seguir. 
  De modo geral, é possível notar que a série não apresenta um comportamento estável ao 
  longo do tempo. Inicialmente, há um aumento no valor da variável até 1983 e, 
  na sequência, há uma evidente tendência de decrescimento da variável ao longo do 
  tempo. Porém, a taxa de mudança nos valores da variável entre anos consecutivos é 
  pequena a partir de 1988. 
  Logo, os valores da variável se mostram bastante estáveis após 1990.

```{r, echo=FALSE, fig.align='center', fig.height=5, fig.pos="H"}

# Gráfico da série temporal
par(mar=c(4,4,0.5,0));plot(cpg, xlab = "Tempo", ylab = "Preço mediano anual")

```



- b) O ajuste do modelo de regressão linear com a variável resposta transformada pelo 
  logarítmo está apresentada na sequência. 
  É possível notar que a transformação aplicada aos dados linearizou a relação da 
  variável resposta e o tempo. 
  Assim, o modelo de regressão linear apresentou um ajuste satisfatório aos dados e 
  com elevado valor de coeficiente de determinação ajustado.

```{r, echo=FALSE, fig.align='center', fig.height=5, fig.pos="H"}

# Ajuste do modelo
da <- data.frame("cpg" = cpg, "t" = 1980:2008)
fit <- lm(I(log(cpg)) ~ t, data = da)
summary(fit)

# Gráfico da série temporal e do modelo ajustado
par(mar=c(4,4,0,0));plot(log(da$cpg[1:29]) ~ da$t, type = "l", col = "black", 
     xlab = "Tempo", ylab = "Preço mediano anual")
abline(reg = fit, col = "red")
legend(2000, 10, 
       lty = c(1, 1), 
       col = c("black", "red"), 
       legend = c("Dados", "Ajuste"), 
       bty = "n", 
       cex = 1)

```



- c) O gráfico Half-Normal plot sugere que o modelo está bem especificado e ajustado 
  aos dados, uma vez que há apenas uma observação fora dos envelopes simulados. 
  Porém, o gráfico de dispersão dos resíduos ordinários em relação ao tempo mostra 
  uma variância não constante ao longo do tempo, com um padrão cúbico dos resíduos. 
  Esse resultado sugere a possibilidade de incluir a covariável tempo com seu efeito 
  de terceiro grau, isto é, o tempo pode ser inserido com uma especificação polinomial 
  de grau três no preditor linear. 
  O histograma dos resíduos indicou normalidade, a qual foi confirmada pelo teste de 
  normalidade de Shapiro-Wilk, em que a hipótese nula não foi rejeitada, para nível de 
  5% de significância. 
  Por fim, o gráfico de autocorrelação dos resíduos indicou que há baixa correlação 
  temporal.

```{r, echo=FALSE, fig.align='center', fig.height=4, fig.pos="H", warning=FALSE, message=FALSE}

# Half-Normal plot
library(hnp)
par(mar=c(4,4,0,0));hnp(fit)

# Resíduos ~ covariável
par(mar=c(4,4,0,0));plot(residuals(fit) ~ da$t)
abline(h = 0, lty=2)

# Histograma dos resíduos
shapiro.test(residuals(fit))
par(mar=c(4,4,2,0));hist(residuals(fit))

# Autocorrelação dos resíduos
par(mar=c(4,4,3,0));acf(residuals(fit), lag.max = 29)

```



- d) O modelo de regressão ajustado para erros autocorrelacionados foi o SARIMA(0,0,2), 
isto é, o modelo ajustado foi um SARIMA com médias móveis de ordem dois. 
A figura apresentada na sequência mostra que os resíduos padronizados estão estáveis 
a partir do ano de 1990 entre anos consecutivos. 
Ainda, os resíduos do modelo ajustado se mostram não correlacionados. 
Também é possível notar que a distribuição dos resíduos se aproxima de uma distribuição 
normal, com apenas dois valores extremos no gráfico qqplot. 
Por fim, é possível observar que o p-valor do teste Ljung-Box aumenta em função da 
defasagem, sugerindo que a hipótese nula de que os resíduos são independentes e 
identicamente distribuídos não é rejeitada para grande valores de defasagem.


```{r, echo=FALSE, fig.align='center', fig.height=5, fig.pos="H", warning=FALSE, message=FALSE}

# Resumo do ajuste e dos resíduos
sarima(xdata = da$cpg, p = 0, d = 0, q = 2, xreg = cbind(da$t))

```





# PARTE IV - Análise Espectral e Filtragem

## Exercício 6a

- a)

```{r, echo=FALSE, fig.align='center', fig.height=5, fig.pos="H", warning=FALSE, message=FALSE}

require(astsa)

data(climhyd)
data <- subset(x = climhyd, select = c('DewPt', 'Inflow'))
data$DewPt <- sqrt(data$DewPt)
data$Inflow <- log(data$Inflow)
data <- ts(data, frequency = 1)

plot.ts(data)

(m1 <- arima(x = data[,1], order = c(0,0,0), seasonal = list(order = c(0,1,1), period = 12)))
(m2 <- arima(x = data[,2], order = c(0,0,0), seasonal = list(order = c(0,1,1), period = 12)))

```





## Exercício 6b

- b) Os resultados encontrados sugerem três tipos de comportamento para as séries 
  analisadas. Inicialmente, há correlação cruzada negativa, a qual vai diminuindo até 
  não haver correlação. Posteriormente, há correlação cruzada positiva, seguido de 
  correlação cruzada negativa. 
  Os comportamentos distintos podem ser atribuídos a sazonalidade dos dados.

```{r, echo=FALSE, fig.align='center', fig.height=5, fig.pos="H", warning=FALSE, message=FALSE}

require(astsa)

par(mar=c(4,4,0,0));ccf2(x = ts(m1$residuals), y =ts(m2$residuals), main = c("Cross Correlation Plot"))

```




